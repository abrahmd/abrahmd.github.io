<!doctype html>
<html lang="en">
<head>
  <script>
    window.MathJax = { tex:{ inlineMath:[['$','$'],['\\(','\\)']] }, svg:{ fontCache:'global' } };
  </script>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title> Abrahm DeVine Computer Vision Projects </title>
  <style>
    :root{ --bg:#0f1220; --panel:#15192b; --text:#e9edf6; --muted:#a7b0c7; --brand:#4f8bff; --border:#242b42; }
    @media (prefers-color-scheme: light){ :root{ --bg:#f6f8ff; --panel:#ffffff; --text:#0d1320; --muted:#5c657a; --brand:#2563eb; --border:#e3e7f2; } }

    *{box-sizing:border-box}
    html,body{height:100%}
    body{margin:0; font-family: ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial; background:var(--bg); color:var(--text); line-height:1.55}
    a{color:inherit}

    .container{max-width:1200px; margin:0 auto; padding:20px}

    /* 3-column layout */
    .grid{display:grid; grid-template-columns: 220px 1fr 260px; gap:24px}
    @media (max-width: 1024px){ .grid{grid-template-columns: 200px 1fr} .toc{display:none} }
    @media (max-width: 720px){ .grid{grid-template-columns: 1fr} .projects{order:-1} }

    .box{background:var(--panel); border:1px solid var(--border); border-radius:14px; padding:14px}
    .aside-title{margin:0 0 8px 0; font-size:1rem; opacity:.9}

    /* Left: projects list (vertical tabs) */
    .projects{position:sticky; top:16px; align-self:start}
    .project-nav{display:flex; flex-direction:column; gap:8px}
    .project-btn{appearance:none; border:1px solid var(--border); background:transparent; color:var(--muted); padding:10px 12px; border-radius:10px; text-align:left; cursor:pointer; font-weight:600}
    .project-btn[aria-selected="true"]{color:var(--text); background:rgba(79,139,255,.15); box-shadow:0 0 0 2px rgba(79,139,255,.3) inset}

    /* Center: content panels */
    .panel{display:none}
    .panel.active{display:block}
    .paper{background:var(--panel); border:1px solid var(--border); border-radius:16px; padding:18px}
    h1,h2,h3{scroll-margin-top:80px}
    h2{margin-top:18px}
    .meta{color:var(--muted); margin:2px 0 14px}

    /* Right: TOC */
    .toc{position:sticky; top:16px; align-self:start}
    .toc ul{list-style:none; padding:0; margin:0; display:grid; gap:6px}
    .toc a{display:block; color:var(--muted); text-decoration:none; border-radius:8px; padding:6px 8px}
    .toc a:hover{background:rgba(79,139,255,.15); color:var(--text)}
    .toc .lvl2{padding-left:8px}
  </style>
</head>
<body>
  <div class="container">
    <header style="margin-bottom:12px">
      <h1 style="margin:0"> Abrahm DeVine Computer Vision projects </h1>
    </header>

    <div class="grid">
      <!-- LEFT: Projects (vertical tabs) -->
      <aside class="projects" aria-label="Projects">
        <div class="box">
          <h3 class="aside-title">Projects</h3>
          <nav class="project-nav" role="tablist" aria-orientation="vertical">
            <button class="project-btn" role="tab" data-target="#p1" aria-controls="p1" aria-selected="true">Project 1</button>
            <button class="project-btn" role="tab" data-target="#p2" aria-controls="p2" aria-selected="false">Project 2</button>
          </nav>
        </div>
      </aside>

      <!-- CENTER: Panels -->
      <main>
        <article id="p1" class="panel active" role="tabpanel" aria-labelledby="p1-tab" tabindex="0">
          <div class="paper">
            <h1> Images of the Russian Empire </h1>
            <p class="meta"> UC Berkeley Fall 2025 â€¢ Topics: Image Allignment, Gaussian and Laplacian Pyramids, Anti-aliasing, Normalized Cross-Correlation </p>

            <h2>Introduction</h2>
            <p> This project is inspired by Russian photographer Sergei Mikhailovich Prokudin-Gorskii (1863-1944) and his dream of seeing photos of Russia in color. He shot three exposures of several Russian scenes onto
            glass plates using a red, green, and blue filter. The aim of this project is utilize Gaussian and Laplacian pyramids to align these images to create a single clear and colorful photo. As such, the constrution of a historical truth 
            is in the hands of the computer scientist. </p>

            <h2>Methodology</h2>
            <h3>Single-Scale Version</h3>
            <p> The first step was to considers metric for similarity between two photos. The black and white images only contain the brightness values accross three channels, and of course each color is not equally bright. Below are lower 
              resolution jpg images where I tested the Normalized Cross-Correlation (equation below), or the normalized dot product between two images, as a reliable metric. Using the green filtered image as a static reference, I shifted (np.roll()) every pixel in the red and blue filtered images
              over a range of -15 to 15 in both the x and y directions, maximizing the NCC of the shifted image and the green image over all possible shifts. The results, shown below, were sufficiently aligned to confirm the of NCC as a reasonable metric and the runtime was less than a second.
            </p>
                 <figure class="eq">
                     <figcaption> Normalized Cross-Correlation (NCC)</figcaption>
                     <p>
                      $$\mathrm{NCC}(I,J)=
                      \frac{\sum_{x,y}\big(I(x,y)-\bar I\big)\big(J(x,y)-\bar J\big)}
                      {\sqrt{\sum_{x,y}\big(I(x,y)-\bar I\big)^2}\,
                      \sqrt{\sum_{x,y}\big(J(x,y)-\bar J\big)^2}$$
                    </p>
              </figure>

            <figure class="media">
               <img src="Project1/out_path_jpgs/cathedral.jpg.jpg" alt="Reconstructed color image" width="350" height="320" loading="lazy" decoding="async">
               <figcaption> Reconstructed color jpg cathedral image, reconstructed by shifting the red filtered image (7,1) for axis=(0,1) and the blue filtered image (-5,-2) for axis=(0,1). </figcaption>
            </figure>

            <figure class="media">
               <img src="Project1/out_path_jpgs/monastery.jpg" alt="Reconstructed color image" width="350" height="320" loading="lazy" decoding="async">
               <figcaption> Color jpg Monastery image, reconstructed by shifting the red filtered image (6, 1) for axis=(0,1) and the blue image (3, -2) for axis=(0,1).  </figcaption>
            </figure>

            <figure class="media">
               <img src="Project1/out_path_jpgs/tobolsk.jpg.jpg" alt="Reconstructed color image" width="350" height="320" loading="lazy" decoding="async">
               <figcaption> Color jpg Tobolsk image, reconstructed by shifting the red filtered image (4,1) for axis=(0,1) and the blue filtered image (-3,-3) for axis=(0,1). </figcaption>
            </figure>

            
            
            <h3> Tackling High-Res Images with Pyramid-Speedup </h3>
            <p> For higher-resolution images, a 15x15 pixel window is not sufficient to allign images properly and a larger window becomes increasingly computationally expensive. By creating a Gaussian pyramid, this same exhausted search can be 
              performed on a smaller image with an even smaller window to search. Small shifts in downsampled images translate to larger shifts in the original large image relative to the downsizing factor, in this case 2. In my implementation, I searched an 8x8 pixel window at each level of 
              the pyramid, which proved to be sufficient at aligning the images properly and in roughly 15 seconds per image. This parameter could potentially be smaller and/or decreased while traversing the pyramid. I also tried the 
              same implementation on a Laplacian pyramid, but the Gauassian pyramid imlementation slightly outperformed (higher NCC score) the Laplacian for all images.
              </p>

            <figure class="media">
               <img src="Project1/out_path_jpgs/Laplace_church.jpg" alt="Reconstructed color image" width="350" height="320" loading="lazy" decoding="async">
               <figcaption> Church. Red shift=(33,-7) for axis=(0,1) and NCC score 0.82. Blue shift = (-25,0) for axis(0,1) and NCC score 0.85. </figcaption>
            </figure>

            <figure class="media">
               <img src="Project1/out_path_jpgs/Laplace_emir.jpg" alt="Reconstructed color image" width="350" height="320" loading="lazy" decoding="async">
               <figcaption> Emir. Red shift=(57,17) for axis=(0,1) and NCC score 0.69. Blue shift = (-49,-24) for axis(0,1) and NCC score 0.67. </figcaption>
            </figure>

            <figure class="media">
               <img src="Project1/out_path_jpgs/Laplace_harvesters.jpg" alt="Reconstructed color image" width="350" height="320" loading="lazy" decoding="async">
               <figcaption> Harvesters Red shift=(65,-3) for axis=(0,1) and NCC score 0.85. Blue shift = (-60,-17) for axis(0,1) and NCC score 0.79. </figcaption>
            </figure>

            <figure class="media">
               <img src="Project1/out_path_jpgs/Laplace_icon.jpg" alt="Reconstructed color image" width="350" height="320" loading="lazy" decoding="async">
               <figcaption> Icon Red shift=(48,5) for axis=(0,1) and NCC score 0.88. Blue shift = (-40,-17) for axis(0,1) and NCC score 0.77. </figcaption>
            </figure>

            <figure class="media">
               <img src="Project1/out_path_jpgs/Laplace_lugano.jpg" alt="Reconstructed color image" width="350" height="320" loading="lazy" decoding="async">
               <figcaption> Lugano Red shift=(52,-12) for axis=(0,1) and NCC score 0.92. Blue shift = (-41,16) for axis(0,1) and NCC score 0.82. </figcaption>
            </figure>

            <figure class="media">
               <img src="Project1/out_path_jpgs/Laplace_melons.jpg" alt="Reconstructed color image" width="350" height="320" loading="lazy" decoding="async">
               <figcaption> Melons Red shift=(96,3) for axis=(0,1) and NCC score 0.94. Blue shift = (-82,-9) for axis(0,1) and NCC score 0.87. </figcaption>
            </figure>

            <figure class="media">
               <img src="Project1/out_path_jpgs/Laplace_self_portrait.jpg" alt="Reconstructed color image" width="350" height="320" loading="lazy" decoding="async">
               <figcaption> Self Portrait Red shift=(98,8) for axis=(0,1) and NCC score 0.70. Blue shift = (-78,-28) for axis(0,1) and NCC score 0.71. </figcaption>
            </figure>


            <figure class="media">
               <img src="Project1/out_path_jpgs/Laplace_siren.jpg" alt="Reconstructed color image" width="350" height="320" loading="lazy" decoding="async">
               <figcaption> Siren Red shift=(46,-18) for axis=(0,1) and NCC score 0.85. Blue shift = (-49,7) for axis(0,1) and NCC score 0.82. </figcaption>
            </figure>

            <figure class="media">
               <img src="Project1/out_path_jpgs/Laplace_three_generations.jpg" alt="Reconstructed color image" width="350" height="320" loading="lazy" decoding="async">
               <figcaption> Three Generations Red shift=(58,-1) for axis=(0,1) and NCC score 0.93. Blue shift = (-54,-11) for axis(0,1) and NCC score 0.86. </figcaption>
            </figure>
            
            
            

            <h2>Contrast</h2>
            <p>
              Below are two interesting images both with and without added contrast. The added contrast function works by taking high and low percentiles for each channel and linearly mapping them to 0 and 1 brightness intensity, thus
              ensuring that every channel spans a full range of brightness. The result is an image with more dark and light pixels and thus more contrast. A dull, flat image for example would have a much more drastic transformation.
              
              <figure class="media">
               <img src="Project1/out_path_jpgs/ex1.jpg" alt="Reconstructed color image" width="350" height="320" loading="lazy" decoding="async">
               <figcaption> Large coconut-like object with no added contrast </figcaption>
            </figure>

            <figure class="media">
               <img src="Project1/out_path_jpgs/ex1_contrast.jpg" alt="Reconstructed color image" width="350" height="320" loading="lazy" decoding="async">
               <figcaption> Large coconut-like object with added contrast </figcaption>
            </figure>

            <figure class="media">
               <img src="Project1/out_path_jpgs/ex2.jpg" alt="Reconstructed color image" width="350" height="320" loading="lazy" decoding="async">
               <figcaption> Waterfall with no added contrast </figcaption>
            </figure>

            <figure class="media">
               <img src="Project1/out_path_jpgs/ex2_contrast.jpg" alt="Reconstructed color image" width="350" height="320" loading="lazy" decoding="async">
               <figcaption> Waterfall with added contrast </figcaption>
            </figure>
              
            </p>

            <h2> Automatic white balance - shift to grey </h2>
            <p>

              Below are the same two images with an additional white balancing added to them. This feature works by finding the mean of each color channel and then multiplying each channel by some value such that their means are all equal, thus
              making each color equally intense on average. This is an assumption that I do not think is fair to make in most lighting conditions, and in these two photos it adds an unnatural looking saturation effect.
              
              <figure class="media">
               <img src="Project1/out_path_jpgs/ex1.jpg" alt="Reconstructed color image" width="350" height="320" loading="lazy" decoding="async">
               <figcaption> Large coconut-like object with no added contrast </figcaption>
            </figure>

            <figure class="media">
               <img src="Project1/out_path_jpgs/ex1_contrast.jpg" alt="Reconstructed color image" width="350" height="320" loading="lazy" decoding="async">
               <figcaption> Large coconut-like object with added contrast </figcaption>
            </figure>

            <figure class="media">
               <img src="Project1/out_path_jpgs/ex2.jpg" alt="Reconstructed color image" width="350" height="320" loading="lazy" decoding="async">
               <figcaption> Waterfall with no added contrast </figcaption>
            </figure>

            <figure class="media">
               <img src="Project1/out_path_jpgs/ex2_contrast.jpg" alt="Reconstructed color image" width="350" height="320" loading="lazy" decoding="async">
               <figcaption> Waterfall with added contrast </figcaption>
            </figure>
                
            </p>

            <h2> Gradient based alignment </h2>
            <p> Rather than assume color channels should be equal for alignment (which, based on the saturation that occurs in the white balancing above, is not a good assumption), we can change our features and consider instead the NCC score of the gradients and thus
              the change in color. This has more potential to align shadows and natural changes in light that affect all colors, and should work better on images that contain a lot of red, green or blue. However, when I ran my implementation,
              the results were surprisingly worse than the original implementation. 

              <figure class="media">
               <img src="Project1/out_path_jpgs/church_gradient.jpg" alt="Reconstructed color image" width="350" height="320" loading="lazy" decoding="async">
               <figcaption> Church aligned by the NCC score of the gradients. Red shift=(33,-8) for axis=(0,1) and NCC score 0.82. Blue shift = (-25,-4) for axis=(0,1) and NCC score 0.84.  Compared with the original Church image above, 
               this set up underperforms slightly. To the naked eye, it hardly distinguishable. </figcaption>
            </figure>
              
            </p>

            <h2> Automatic cropping </h2>
            <p> For this task, I attempted to detect lines at the borders of the complete rgb image. Visually, these lines appear black, red, green, or blue (or the inverse of rgb). I expected to find entire rows or columns to have all channels 
              below some threshold (black borders), have one channel below some threshold (inverse rgb borders), or have two channels below a threshold (rgb borders). However, I was unable to get this approach up and running succesfully. Perhaps an
              alternate approach or a bug-free implementation is required. I instead employed an automatic crop of 5% off each edge before aligning the images with the hope that this would not only cause better alignment but reduce left over
              borders. Regardless, most of the images alligned have some sort of irregular border that I think holds its own beauty and nostalgia for vintage aesthetics and the imperfection of film photographs. After all, no matter how many modern
              tricks we can apply to images, there remains an impossibility of grasping truth. The image will always be a ghostly and artificial attempt at capturing what is already lost or creating what was never there to begin with. 

              
            </p>
          </div>
        </article>

        <article id="p2" class="panel" role="tabpanel" aria-labelledby="p2-tab" tabindex="0">
          <div class="paper">
            <h1>CS 280A Project 0: Your Title</h1>
            <p class="meta">Author â€¢ Semester â€¢ Tools: scikit-image, Matplotlib</p>

            <h2>Introduction</h2>
            <p>Problem summary and contribution.</p>

            <h2>Methodology</h2>
            <h3>Core Method</h3>
            <p>Key ideas and algorithm choices.</p>
            <h3>Variants & Ablations</h3>
            <p>What variations you tried and findings.</p>

            <h2>Results</h2>
            <p>Qualitative and quantitative outcomes.</p>

            <h2>Lessons Learned</h2>
            <p>Trade-offs, improvements, future work.</p>
          </div>
        </article>
      </main>

      <!-- RIGHT: On this page (TOC) -->
      <aside class="toc" aria-label="On this page">
        <div class="box">
          <h3 class="aside-title">On this page</h3>
          <ul id="toc-list"></ul>
        </div>
      </aside>
    </div>
  </div>

  <script>
    // Vertical tabs logic
    const buttons = Array.from(document.querySelectorAll('.project-btn'));
    const panels = Array.from(document.querySelectorAll('.panel'));

    function showPanel(targetId){
      panels.forEach(p=>p.classList.toggle('active', '#' + p.id === targetId));
      buttons.forEach(b=>b.setAttribute('aria-selected', String(b.dataset.target === targetId)));
      buildTOC();
      const p = document.querySelector(targetId);
      if(p) { p.focus({preventScroll:true}); history.replaceState(null, '', targetId); }
    }

    document.querySelector('.project-nav').addEventListener('click', (e)=>{
      const btn = e.target.closest('.project-btn'); if(!btn) return; showPanel(btn.dataset.target);
    });

    document.querySelector('.project-nav').addEventListener('keydown', (e)=>{
      const i = buttons.indexOf(document.activeElement); if(i<0) return;
      if(e.key==='ArrowDown'){ e.preventDefault(); buttons[(i+1)%buttons.length].focus(); }
      if(e.key==='ArrowUp'){ e.preventDefault(); buttons[(i-1+buttons.length)%buttons.length].focus(); }
      if(e.key==='Home'){ e.preventDefault(); buttons[0].focus(); }
      if(e.key==='End'){ e.preventDefault(); buttons[buttons.length-1].focus(); }
      if(e.key==='Enter' || e.key===' '){ e.preventDefault(); const btn=document.activeElement; showPanel(btn.dataset.target); }
    });

    // Build right-hand TOC from active panel h2/h3
    function slugify(str){ return str.toLowerCase().trim().replace(/[^a-z0-9]+/g,'-').replace(/(^-|-$)/g,''); }
    function buildTOC(){
      const toc = document.getElementById('toc-list'); toc.innerHTML='';
      const active = document.querySelector('.panel.active'); if(!active) return;
      const headers = active.querySelectorAll('h2, h3');
      headers.forEach(h=>{ if(!h.id) h.id = active.id + '-' + slugify(h.textContent); });
      headers.forEach(h=>{
        const li = document.createElement('li'); if(h.tagName==='H3') li.className='lvl2';
        const a = document.createElement('a'); a.href = '#' + h.id; a.textContent = h.textContent; li.appendChild(a); toc.appendChild(li);
      });
    }

    // Smooth scrolling for TOC links
    document.addEventListener('click', (e)=>{
      const a = e.target.closest('a[href^="#"]'); if(!a) return;
      const id = a.getAttribute('href').slice(1); const target = document.getElementById(id);
      if(target){ e.preventDefault(); target.scrollIntoView({behavior:'smooth', block:'start'}); }
    });

    // Initialize from hash or default
    (function init(){
      const hash = location.hash || '#p1';
      const found = panels.some(p => '#' + p.id === hash);
      showPanel(found ? hash : '#p1');
    })();
  </script>
</body>
</html>
