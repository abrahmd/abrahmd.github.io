<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Agentic AI Evaluation Agent</title>
  <link rel="stylesheet" href="../../css/style.css" />
  <style>
    .page-content {
      padding: 34px 0;
    }
    .page-content h1 {
      margin: 0 0 8px 0;
      font-size: 2.2rem;
      letter-spacing: -0.03em;
    }
    .page-content h2 {
      margin: 32px 0 14px 0;
      font-size: 1.15rem;
      letter-spacing: -0.02em;
    }
    .page-content p {
      margin: 0 0 18px 0;
      color: var(--text);
      line-height: 1.55;
    }
    .page-content strong {
      font-weight: 600;
      color: var(--text);
    }
  </style>
</head>

<body>
  <header class="site-header">
    <div class="container">
      <nav class="top-nav">
        <a class="brand" href="../../index.html">Home</a>
        <div class="nav-links">
          <a href="../engineering.html">Engineering</a>
          <a href="../swimming.html">Swimming</a>
          <a href="../film.html">Film</a>
        </div>
      </nav>
    </div>
  </header>

  <main class="container">
    <div class="page-content">
      <h1>Agentic AI Evaluation Agent (Green Agent)</h1>

      <article>
        <section id="project-overview">
          <h2>Problem / Motivation</h2>
          <p>
            In this project, I built an evaluation agent that can be used to test the behavior of other embodied agents completing tasks in a simulated environment (ALFWorld).
            While many agentic AI systems are evaluated using static benchmarks or single-turn tasks, different metrics need to be considered for an agent that moves in a physical environment. 
            Existing evaluations focus on narrow metrics that do not capture the scope of a, potentially robotic agent, completing a task in the home. Beyond completing a task,
            an agent should respect the physical environment and the objects in it. Closing doors, shelves, the refrigerator, turning off the stove, making 
            excessive movements etc. should be considered. Some of these actions (or lack of actions) should be penalized more severly than others.
            This project focuses on building a green evaluation agent that builds upon existing benchmarks by considering environment interaction into the evaluation process. The agent
            is also hosted on AgentBeats, enabling large-scale and reproducible assessments of agentic performance.</p>

          <h2>Goal and Success Criteria</h2>
          <p>The goal was to implement an evaluation agent capable of running standardized, repeatable assessments of autonomous agents in a simulated environment. Success was defined by the ability to reliably execute evaluation episodes, capture detailed interaction traces, and produce consistent results across runs using a common agent interface.</p>

          <h2>Constraints</h2>
          <p>The evaluation agent needed to conform to the AgentBeats protocol so it could interoperate with other agents without custom adapters. Evaluations had to be reproducible across machines, requiring strict control over environment setup and execution. Additionally, agent behavior depended on external LLM APIs, introducing constraints around latency, cost, and determinism.</p>

          <h2>Approach and Key Decisions</h2>
          <p>I implemented the green agent as a containerized service that exposes standardized endpoints for task execution and reporting. A key design decision was to separate evaluation logic from agent under test behavior, allowing the same evaluator to be reused across different agents. Docker was used to ensure isolation and reproducibility, while scripted orchestration ensured that evaluations could be launched deterministically.</p>

          <h2>Implementation</h2>
          <p>The green agent is implemented in Python and packaged as a Docker container compatible with the AgentBeats backend. 
            It integrates with a simulated environment (e.g., ALFWorld) to issue tasks, observe agent actions, and track outcomes over multi-step episodes.
             Evaluation runs are coordinated through orchestration scripts that launch the backend, register agents, and collect logs and traces for analysis.</p>

          <h2>Results</h2>
          <p>The implemented green agent successfully runs end-to-end evaluations of autonomous agents, capturing detailed interaction histories and final outcomes. It demonstrates that agent behavior can be evaluated in a controlled, repeatable manner across machines and configurations using a standardized interface.</p>

          <h2>Evaluation and Insights</h2>
          <p>Building the evaluation agent highlighted the importance of protocol-driven design for agent interoperability. Containerization proved essential for reproducibility, while structured logging enabled meaningful post-hoc analysis of agent decisions. The project also revealed practical challenges in evaluating agent quality beyond binary success metrics, motivating richer behavioral analysis.</p>

        </section>
      </article>
    </div>
  </main>

  <footer class="footer">
    <div class="container">
      <div class="footer-inner">
        <span><span id="year"></span> Abrahm DeVine</span>
        <div class="footer-links">
          <a href="https://www.linkedin.com/">LinkedIn</a>
          <a href="https://github.com/">GitHub</a>
          <a href="mailto:you@email.com">Email</a>
        </div>
      </div>
    </div>
  </footer>

  <script>
    document.getElementById("year").textContent = new Date().getFullYear();
  </script>
</body>
</html>
