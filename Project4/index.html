
<main class="page">
    <style>
        .gallery {
        display: grid;
        grid-template-columns: repeat(2, 1fr); /* 2 columns */
        gap: 10px; /* space between images */
        max-width: 600px;
        margin: auto;
        }
        .gallery img {
        width: 100%;
        height: auto;
        border-radius: 6px;
        }
        .line {
        display: grid;
        grid-template-columns: repeat(3, 1fr); /* 3 images per row */
        gap: 12px;
        max-width: 960px;
        margin: 2rem auto;
        }
        .line img {
        width: 100%;
        height: 400px;       /* same height for all */
        object-fit: contain;   /* crop to fill without distortion */
        border-radius: 8px;
        display: block;
        }
        .line2 {
        display: grid;
        grid-template-columns: repeat(2, 1fr); /* 2 images per row */
        gap: 12px;
        max-width: 1700px;
        margin: 2rem auto;
        }
        .line2 img {
        width: 100%;
        height: 100%;       /* same height for all */
        object-fit: contain;   /* crop to fill without distortion */
        border-radius: 8px;
        display: block;
        }
        .line5 {
        display: grid;
        grid-template-columns: repeat(6, 1fr); 
        gap: 12px;
        width: 2000px;
        }
        .line5 img {
        width: 100%;
        height: 100%;       /* same height for all */
        object-fit: contain;   /* crop to fill without distortion */
        border-radius: 8px;
        display: block;
        }
        .pixel-patch {
        /* turn off smoothing */
        image-rendering: pixelated;     /* modern browsers */
        image-rendering: crisp-edges;   /* fallback */

        /* make sure you scale by an integer multiple */
        width: 128px;   /* 8 * 16 */
        height: 128px;
        }
    </style>

    <h1> Project 4 Part 0 - Calibrating the camera and capturing a 3d scan </h1>

    <figure class="line2">
        <img src = "data/Viser1.png">
        <img src = "data/Viser2.png">
    </figure>

    <p> In the above images, we see two visualizations of the same 30+ photos of my favorite pencil holder. We see the camera frustrum poses and the resulting images. </p>

    <h1> Part 1 - Fitting a Neural Field to a 2d Image </h1>

    <p> 
        Here we construct a Neural Field (F: {u, v} -> {r, g, b}) that can represent a 2d image in the nearal network space. <br>
        We first build a multilayer perceptron with an input of a 42 dimensional sinusoidal positional encoding of the (x,y) coordinates, 6 hidden layers of 256 neurons each <br>
        and each followed by a ReLU activation function, and the sigmoid function mapping predictions to a 3D RGB output. This network was trained using the Adam optimizer, <br>
        a learning rate of 1e-3, and 2000 iterations with a batch size of 10k in the training loop. The loss function used was MSE loss. <br>
        Below we see two images learned by the network. Each image is reconstructed below after 0, 100, 200, 1000, and 2000 iterations. <br>
    </p>

        <figure class="line5">
            <img src = 'data/fox_0.jpg'>
            <img src = 'data/fox_100.jpg'>
            <img src = 'data/fox_200.jpg'>
            <img src = 'data/fox_1000.jpg'>
            <img src = 'data/fox_1999.jpg'>
        </figure>

        <figure class="line5">
            <img src = 'data/lazaro_0.jpg'>
            <img src = 'data/lazaro_100.jpg'>
            <img src = 'data/lazaro_200.jpg'>
            <img src = 'data/lazaro_1000.jpg'>
            <img src = 'data/lazaro_1999.jpg'>
        </figure>

        <p> Below Peak Signal-to-Noise Ratio (PSNR) graphed over 2000 training iterations of the second image.  It seems to plateau around 1750 iterations <br>
            but reaches a reasonable result much sooner, even around 250 iterations.

        </p>

        <img src = "data/psnr_curve.png" width="1000px">

    <p> Below we see the effects of altering the width of the model and the positional encoding frequency. From left to right top to bottom, the four images show
        width=20 & L=0, width=20 & L=4, width = 100 & L = 0, width = 100 & L = 4. Each image is shown on the 2000th iteration. <br>
        It seems that adding dimensions to the input in the positional encoding produces reasonably clear results before adding width to the model. In both instances
        where L=0, we see the blurriest images.
     </p>

        <figure class = "gallery">
            <img src = 'data/lazaro_L0W20_1999.jpg'>
            <img src = 'data/lazaro_L4W20_1999.jpg'>
            <img src = 'data/lazaro_L0W100_1999.jpg'>
            <img src = 'data/lazaro_L4W100_1999.jpg'>
        </figure>
    
    <h1> Part 2 - Fitting a Neural Radiance Field from Multi-view Images </h1>

    In this section I fit a neural radiance field from a predefined lego dataset. To implement this from images taken from a calibrated camera, pixels need to be
    converted first to camera coordinates, world coordinates, and finally into rays.
    The pixel to camera conversion takes place by using the  inverse of the instrinsics matrix to transform the homogenous pixel coordinate to 3D space. We then multiply
    this result by the inverse of the extrinsic matrix to get this coordinate in world coordinates (defined by the Aruco tags in the images I captured). For this same pixel,
    we can define a ray with the camera center (given as the translation component in the camera-to-world matrix) the pixel in world at a depth of one from the camera center. 
    The MLP is constructed with the architecture below,

    <img src = "model_architecture.png"> <br>

    Then, to train the nueral radiance field, we sample rays accross all images and sample points along these rays, randomly dispersed between some distance interval. For 
    the lego scence, this interval is between 2 and 6 meters. Below is a visualization of this sampling method accross many images and from only one camera.

    <figure class = "gallery">
        <img src = 'frustrums2.png'>
        <img src = 'frustrums1.png'>
    </figure>

    As we train the NeRF, the network learns the color densities at points in space. Below is a rendered perspective from the validation set at 0 iterations of training, 500
    iterations, 4,000 iterations, adn 9500 iterations.

    <figure class = "gallery">
        <img src = 'results/rendered_image_0.jpg'>
        <img src = 'results/rendered_image_500.jpg'>
        <img src = 'results/rendered_image_4000.jpg'>
        <img src = 'results/rendered_image_9500.jpg'>
    </figure>

    Below we see the MSE and PSNR curves generated during training.

        <img src = 'results/mse_plot.png'>
        <img src = 'results/psnr_plot.png'>
    <br>
    And finally, using the spherical perspectives from the test set, we can generate a 3D view of the lego truck. <br>

    <img src = "test_results.gif">

    <br>
    Now, we can repeat the process on data collected from an iPhone camera, using Aruco tags to calibrate the camera. I tried to capture photos within 1 meter of the teapot,
    so I chagned my near/far parameters to be 0.2-1.0. I also sampled 64 points per ray as opposed to 32. 
    Below is a rendered perspective from the validation set at
    0 iterations of training, 500 iterations, 4,000 iterations, adn 8,000 iterations, along with the MSE and PSNR curves. 


    <figure class = "gallery">
        <img src = 'results/rendered_kitty_0.jpg'>
        <img src = 'results/rendered_kitty_500.jpg'>
        <img src = 'results/rendered_kitty_4000.jpg'>
        <img src = 'results/rendered_kitty_8000.jpg'>
    </figure>

    <img src = 'results/mse_plot_kitty.png'>
    <img src = 'results/psnr_plot_kitty.png'>


    <br>
    And the final rendered 3D gif: <br>
    <img src = "teapot2.gif">
</main>