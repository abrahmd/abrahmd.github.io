<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title> Abrahm DeVine – CS 280A Projects</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      }
    };
  </script>
  
  <!-- Load MathJax -->
  <script>
    id="MathJax-script"
    async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>

  <style>
    /* Basic reset */
    * {
      box-sizing: border-box;
      margin: 0;
      padding: 0;
    }

    body {
      font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", sans-serif;
      background-color: #ffffff;
      color: #111827;
      line-height: 1.6;
    }

    a {
      color: inherit;
      text-decoration: none;
    }

    a:hover {
      text-decoration: underline;
    }

    /* Top navigation */

    .top-nav {
      position: sticky;
      top: 0;
      z-index: 10;
      background-color: hsl(0, 0%, 97%);
    }

    .top-nav-inner {
      max-width: 1100px;
      margin: 0 auto;
      padding: 12px 16px;
      display: flex;
      align-items: center;
      gap: 16px;
    }

    .brand {
      font-weight: 600;
      font-size: 0.95rem;
      white-space: nowrap;
    }

    .brand span {
      font-weight: 400;
      color: #6b7280;
      margin-left: 4px;
    }

    .nav-links {
      display: flex;
      gap: 12px;
      font-size: 0.9rem;
      color: #6b7280;
      flex: 1;
      justify-content: center;
      overflow-x: auto;
    }

    .nav-links a {
      white-space: nowrap;
      padding: 2px 4px;
    }

    .nav-links a.active {
      color: #111827;
      font-weight: 500;
    }

    .nav-right {
      display: flex;
      gap: 10px;
      font-size: 0.85rem;
      color: #6b7280;
      white-space: nowrap;
    }

    .nav-right a {
      padding: 2px 4px;
    }

    .nav-right a:hover {
      color: #111827;
    }

    /* Main layout */

    main {
      max-width: 1500px;
      margin: 24px auto;
      padding: 0 16px 40px;

      justify-content: flex-start;
      align-items: flex-start;
      gap: 20px;
      padding: 0 20px;
    }

    .layout {
      display: grid;
      grid-template-columns: minmax(0, 3fr) 220px;
     
      flex: 1;
    }

    /* Article content */

    .article {
      background-color: #ffffff;
      padding: 24px 24px 32px;
      /* no border, no shadow – just white on gray background */
    }

    .article h1 {
      font-size: 1.9rem;
      margin-bottom: 4px;
    }

    .subtitle {
      font-size: 0.9rem;
      color: #6b7280;
      margin-bottom: 20px;
    }

    h2 {
      font-size: 1.3rem;
      margin-top: 24px;
      margin-bottom: 8px;
    }

    h3 {
      font-size: 1.05rem;
      margin-top: 18px;
      margin-bottom: 4px;
    }

    p {
      margin-bottom: 12px;
    }

    ul {
      margin: 0 0 12px 20px;
    }

    li + li {
      margin-top: 4px;
    }

    hr {
      border: none;
      border-top: 1px solid #e5e5e5;
      margin: 18px 0;
    }

    img {
      max-width: 100%;
      height: auto;
      display: block;
      margin: 12px 0;
    }

    /* Sidebar */
    
    
    .sidebar {
        position: fixed;
        top: 65px;          /* adjust to sit under your title */
        right: 700px;         /* distance from right edge */
        width: 230px;
        padding: 16px;
        background-color: #ffffff;
        z-index: 10;
    } 

    .sidebar-title {
      text-transform: uppercase;
      letter-spacing: 0.08em;
      font-size: 0.8rem;
      margin-bottom: 8px;
    }

    .toc {
      list-style: none;
      border-left: 1px solid #e5e5e5;
      padding-left: 10px;
    }

    .toc > li {
      margin-bottom: 6px;
    }

    .toc a {
      color: #2563eb;
      text-decoration: none;
    }

    .toc a:hover {
      text-decoration: underline;
    }

    .toc-sub {
      list-style: none;
      padding-left: 12px;
      margin-top: 4px;
    }

    .toc-sub li {
      margin-bottom: 3px;
    }

    .toc-sub a {
      color: #6b7280;
    }

    .line2 {
        display: grid;
        grid-template-columns: repeat(2, 1fr);  /* three equal columns */
        gap: 10px;                               /* space between images */
        max-width: 400px;                        /* optional, controls total width */
        margin: auto;                            /* center the whole figure */
    }

    .line2 img {
        width: 100%;
        height: auto;
        border-radius: 6px;                      /* optional styling */
    }

    .line2 figcaption {
        grid-column: 1 / -1;                     /* caption spans all three images */
        text-align: center;
        margin-top: 8px;
        font-size: 0.9rem;
        color: #6b7280;
    }


    .line3 {
        display: grid;
        grid-template-columns: repeat(3, 1fr);  /* three equal columns */
        gap: 10px;                               /* space between images */
        max-width: 900px;                        /* optional, controls total width */
        margin: auto;                            /* center the whole figure */
    }

    .line3 img {
        width: 100%;
        height: auto;
        border-radius: 6px;                      /* optional styling */
    }

    .line3 figcaption {
        grid-column: 1 / -1;                     /* caption spans all three images */
        text-align: center;
        margin-top: 8px;
        font-size: 0.9rem;
        color: #6b7280;
    }

    .line4 {
        display: grid;
        grid-template-columns: repeat(4, 1fr);  /* three equal columns */
        gap: 10px;                               /* space between images */
        max-width: 900px;                        /* optional, controls total width */
        margin: auto;                            /* center the whole figure */
    }

    .line4 img {
        width: 100%;
        height: auto;
        border-radius: 6px;                      /* optional styling */
    }

    .line4 figcaption {
        grid-column: 1 / -1;                     /* caption spans all three images */
        text-align: center;
        margin-top: 8px;
        font-size: 0.9rem;
        color: #6b7280;
    }

    .line5 {
        display: grid;
        grid-template-columns: repeat(5, 1fr);  /* three equal columns */
        gap: 10px;                               /* space between images */
        max-width: 900px;                        /* optional, controls total width */
        margin: auto;                            /* center the whole figure */
    }

    .line5 img {
        width: 100%;
        height: auto;
        border-radius: 6px;                      /* optional styling */
    }

    .line5 figcaption {
        grid-column: 1 / -1;                     /* caption spans all three images */
        text-align: center;
        margin-top: 8px;
        font-size: 0.9rem;
        color: #6b7280;
    }

    .line6 {
        display: grid;
        grid-template-columns: repeat(6, 1fr);  /* three equal columns */
        gap: 10px;                               /* space between images */
        max-width: 900px;                        /* optional, controls total width */
        margin: auto;                            /* center the whole figure */
    }

    .line6 img {
        width: 100%;
        height: auto;
        border-radius: 6px;                      /* optional styling */
    }

    .line6 figcaption {
        grid-column: 1 / -1;                     /* caption spans all three images */
        text-align: center;
        margin-top: 8px;
        font-size: 0.9rem;
        color: #6b7280;
    }

    .line7 {
        display: grid;
        grid-template-columns: repeat(7, 1fr);  /* three equal columns */
        gap: 10px;                               /* space between images */
        max-width: 900px;                        /* optional, controls total width */
        margin: auto;                            /* center the whole figure */
    }

    .line7 img {
        width: 100%;
        height: auto;
        border-radius: 6px;                      /* optional styling */
    }

    .line7 figcaption {
        grid-column: 1 / -1;                     /* caption spans all three images */
        text-align: center;
        margin-top: 8px;
        font-size: 0.9rem;
        color: #6b7280;
    }

    

    .gallery {
        display: grid;
        grid-template-columns: repeat(2, 1fr); /* 2 columns */
        gap: 10px; /* space between images */
        max-width: 600px;
        margin: auto;
    }
    
    .gallery img {
        width: 100%;
        height: auto;
        border-radius: 6px;
    }

    .gallery figcaption {
        grid-column: 1 / -1; 
        margin-top: 6px;
        font-size: 0.85rem;
        color: #6b7280;      
        text-align: center;    
    }

    .figure {
        margin: 24px 0;          /* space above and below each image */
        text-align: center;      /* center the caption text */
    }

    .figure img {
        display: block;
        margin: 0 auto;          /* center the image itself */
        max-width: 700px;        /* max actual width of the image */
        width: 100%;             /* shrink on small screens */
        height: auto;
    }

    .figure figcaption {
        margin-top: 6px;
        font-size: 0.85rem;
        color: #6b7280;          /* soft gray, like your subtitle text */
    }

    /* Responsive */

    @media (max-width: 900px) {
      .layout {
        grid-template-columns: minmax(0, 1fr);
      }

      .sidebar {
        position: static;
        margin-top: 16px;
      }
    }

    @media (max-width: 640px) {
      .top-nav-inner {
        flex-wrap: wrap;
      }

      .brand {
        width: 100%;
      }

      .nav-links {
        justify-content: flex-start;
      }
    }

    .nav-icon {
    width: 18px;
    height: 18px;
    margin-right: 4px;
    vertical-align: text-bottom;
    color: #6b7280; /* gray-500 */
    transition: color 0.15s ease;
    }

    .nav-right a:hover .nav-icon {
    color: #111827; /* darker hover */
    }
  </style>
</head>
<body>

  <!-- Top navigation 
  <header class="top-nav">
    <div class="top-nav-inner">
      <div class="brand">
        Abrahm DeVine
        <span> CS 280A Projects </span>
      </div>

      <nav class="nav-links">
        <a href="#home" class="active">Home</a>
        <a href="#proj0">Project 0</a>
        <a href="#proj1">Project 1</a>
        <a href="#proj2">Project 2</a>
        <a href="#proj3">Project 3</a>
        <a href="#proj4">Project 4</a>
        <a href="#proj5">Project 5</a>
      </nav>

      <div class="nav-right">
        <a href="https://github.com/abrahmd" target="_blank">
            <svg class="nav-icon" viewBox="0 0 24 24" fill="currentColor">
              <path fill-rule="evenodd" clip-rule="evenodd"
                d="M12 2C6.48 2 2 6.58 2 12.26c0 4.5 2.87 8.32 6.84 9.68.5.1.68-.22.68-.49
                0-.24-.01-.87-.01-1.71-2.78.62-3.37-1.37-3.37-1.37-.46-1.2-1.11-1.52-1.11-1.52-.91-.64.07-.63.07-.63
                1 .07 1.53 1.07 1.53 1.07.9 1.57 2.36 1.12 2.94.85.09-.67.35-1.12.63-1.38-2.22-.26-4.56-1.14-4.56-5.07
                0-1.12.39-2.03 1.03-2.75-.1-.26-.45-1.3.1-2.71 0 0 .84-.27 2.75 1.05A9.38 9.38 0 0 1 12 6.8c.85.01 1.71.12
                2.51.35 1.9-1.32 2.74-1.05 2.74-1.05.55 1.41.2 2.45.1 2.71.64.72 1.03 1.63 1.03 2.75
                0 3.94-2.34 4.8-4.57 5.06.36.32.68.94.68 1.9 0 1.37-.01 2.47-.01 2.81
                0 .27.18.59.69.49A10.28 10.28 0 0 0 22 12.26C22 6.58 17.52 2 12 2Z" />
            </svg>
            GitHub
          </a>

        <a href="https://www.linkedin.com/in/abrmdvn" target="_blank" rel="noopener noreferrer">LinkedIn</a>

        <a href="https://www.linkedin.com/in/your-handle" target="_blank">
            <svg class="nav-icon" viewBox="0 0 24 24" fill="currentColor" height="18" width="18">
              <path d="M4.98 3.5C4.98 4.88 3.86 6 2.5 6S0 4.88 0 3.5 1.12 1 2.5 1s2.48 1.12 2.48 2.5zM.5 8.25h4V24h-4v-15.75zM8.75 8.25h3.83v2.13h.05c.53-1 1.84-2.13 3.79-2.13 4.06 0 4.81 2.67 4.81 6.14V24h-4v-8.62c0-2.06-.04-4.7-2.86-4.7-2.86 0-3.3 2.23-3.3 4.54V24h-4v-15.75z"/>
            </svg>
            LinkedIn
          </a>
        <a href="https://en.wikipedia.org/wiki/Abrahm_DeVine" target="_blank" rel="noopener noreferrer">Wikipedia</a>
      </div>
    </div>
  </header> -->

  <!-- Main content + sidebar -->
  <main id="home">
    <div class="layout">
      <!-- Article -->
      <article class="article">
        <h1 id= "start"> CS 280A Project 5: Fun With Diffusion Models! </h1>
        <div class="subtitle">
            Abrahm DeVine · December 5, 2025 · UC Berkeley
        </div>
        
        <h1> Part A: The Power of Diffusion Models </h1>
        <p>
            In this section, I experiment with inputs and outputs for DeepFloyd IF, a two-stage diffusion model trained by Stability AI. 
        </p>
        <hr>

        <p>
            First, to get a basic sense of the model, I came up with some prompts and generated their text embeddings using 
            a Huggingface cluster. These are the prompts I generated,

            <ul>
                <li>a high quality photo</li>
                <li>an owl teaching a class of rats</li>
                <li>an orca whale escaping its enclosure at Sea World</li>
                <li>a Chinese city in ruins</li>
                <li>a tired student walking to class in the morning</li>
                <li>a teapot filled with the most enticing tea</li>
                <li>whales kissing</li>
                <li>a photo of whales kissing</li>
                <li>a man waking up to discover that he has been transformed into a cockroach</li>
                <li>a waterfall of blood</li>
                <li>a gorgeous sunflower emerging from ruins</li>
                <li>an oil painting of two whales kissing underwater</li>
                <li>an oil painting of two faces almost touching</li>
                <li>an oil painting of a butterfly wings</li>
                <li>an oil painting of two blooming flowers</li>
                <li>a frowning old man's wrinkly face</li>
                <li>a smiling young woman's face</li>
                <li>a hippo standing on four legs in the desert</li>
                <li>a small house cat standing on four legs</li>
                <li>a jug of milk</li>
                <li>two identical black and white skyscrapers</li>
                <li>a teacher teaching a class full of students</li>
            </ul>

            The first prompt cooresponds to a null embedding. 
            Here are some example outputs from DeepFloyd using a random seed of 100.
       </p>
   
       <figure class="figure">
           <img src="data/generated_image_stage3_10.png" alt="an orca whale escaping its enclosure at Sea World">
           <figcaption> 
               Prompt = "an orca whale escaping its enclosure at Sea World" <br>
               This image was generated with 55 inference steps. Interestingly, the model generated two images of a whale that are similar but distinct.
                The figure standing on the deck is not a very convincing human, and the shape of the pool does not make sense. Furthermore,
                the whale is not escaping, as prompted, but merely jumping out of the water as orcas often do at Sea World. Of all the images, this one
                matches its prompt the least.
           </figcaption>
       </figure>
   
       <figure class="figure">
           <img src="data/generated_image_stage3_2.png">
           <figcaption> 
               Prompt = "an owl teaching a class of rats" <br>
               This image was generated with 100 inference steps. Interestingly, the image is produced in an animated style as opposed to a
                realistic one. Perhaps the absurdity of the prompt led to this result. I also generated this image with fewer inference steps,
                and in comparison, the rats all look very good and consistent. It does resemble a reasonable classroom setting, though the owl's
                returning gaze toward the camera does not exactly suggest ‘teaching.’
           </figcaption>
       </figure>
   
       <figure class="figure">
           <img src="data/generated_image_stage3_7.png">
           <figcaption> 
               Prompt = "a Chinese city in ruins" <br>
               This image was generated with only 20 inference steps.
               Even with fewer inference steps, I like this image a lot. It shows not only ruins and abandonment, but a mysterious fog over the landscape.
               The buildings look reasonably realistic and also beautiful. Only when you zoom in is it clear that the image has faults. 
               I think the artistic liberties of this photo evoke a feeling of loss and mystery and reminds me of my visit to Pompeii. 
           </figcaption>
       </figure>
       
       <p>
            Since I'm least satisfied with the whale image despite a prompt I was excited about, I've asked the model to generate an image on the same prompt,
            this time with num_inference_steps increased to 1,000.
       </p>
       <figure class="figure">
        <img src="data/stage_2_output_whale.png">
        <figcaption> 
            Prompt = "an orca whale escaping its enclosure at Sea World" <br>
            Interestingly, this image is in many ways worse. The saturation is very high, there are mysterious green trails and bright specks in the water,
            there are still mysterious pool shapes happening under the whale, and I see a rather docile whale rather than one trying to escape. 
        </figcaption>
    </figure>
   
    <h1> Part 1: Sampling loops </h1>

    <p> 
        In this section, I implement and modify sampling loops using the pretrained DeepFloyd denoisers to create optical illusions.
    </p>
    
    <h2> 1.1 Implementing the Forward Pass </h2>
   
    <p>
        Below we can see an image of the Berkeley Campanile at progressively higher noise levels by the following equqation,

        $$ q(x_t \mid x_0) = \mathcal{N}\!\left(x_t;\, \sqrt{\bar{\alpha}_t}\, x_0,\; (1 - \bar{\alpha}_t) I \right) $$

        Equivalently, 

        $$ x_t = \sqrt{\bar{\alpha}_t}\, x_0 + \sqrt{1 - \bar{\alpha}_t}\, \epsilon \quad \text{where } \epsilon \sim \mathcal{N}(0,1) $$

        Where $x_t$ is a noisy image at timestep $t$ and $x_0$ is the clean image. 

        We see the original image, followed by versions corrupted at timesteps \( t=250, t=500, \) and  \( t=750. \)
    </p>
       
   
       <figure class="gallery">
           <img src="data/campanile.jpg">
           <img src="data/campanile250.png">
           <img src="data/campanile500.png">
           <img src="data/campanile750.png">
       </figure>
   
       <h2> 1.2 Denoising with Gaussian blur </h2>
   
    <p>
        Below, I implement a naive denoising method. Each noised image is convolved with a 5×5 Gaussian kernel with a standard deviation of 1.5 in both directions. 
       Although this smoothing reduces the apparent noise, it does not fully recover the original image. By \( t=750 \), there remains  only a faint outline of the 
       Campanile.
    </p>
       
   
       <figure class="gallery">
           <img src="data/campanile250.png">
           <img src="data/campanile250_blurred.png">
           <figcaption> Gaussian blur denoising for \( t = 250 \) </figcaption>
       </figure>
   
       <figure class="gallery">
           <img src="data/campanile500.png">
           <img src="data/campanile500_blurred.png">
           <figcaption>  Gaussian blur denoising for \( t = 500 \) </figcaption>
       </figure>
   
       <figure class="gallery">
           <img src="data/campanile750.png">
           <img src="data/campanile750_blurred.png">
           <figcaption>  Gaussian blur denoising for \( t = 750 \) </figcaption>
       </figure>
   
       <h3> 1.3 One-step Denoising </h3>
   
       <p>
        Here I use a pretrained diffusion model (stage_1.unet) to denoise the Campanile image at timesteps $t = 250$, $t = 500$, and $t = 750$. As this is a text-conditioned model,
        we use the embedding for "a high quality photo" to pass to the denoiser.
         For each timestep, we display the original image, the noisy image, and the model’s reconstruction. 
       </p>
       
   
       <figure class="line3">
           <img src="data/campanile.jpg">
           <img src="data/campanile250.png">
           <img src="data/campanile250_denoised.png">
           <figcaption> The original Campanile image, the same image noised at timstep $t = 250$, and the result of 1 step denoising using stage_1.unet. </figcaption>
       </figure>
   
       <figure class="line3">
           <img src="data/campanile.jpg">
           <img src="data/campanile500.png">
           <img src="data/campanile500_denoised.png">
           <figcaption> The original Campanile image, the same image noised at timstep $t = 500$, and the result of 1 step denoising using stage_1.unet. </figcaption>
       </figure>
   
       <figure class="line3">
           <img src="data/campanile.jpg">
           <img src="data/campanile750.png">
           <img src="data/campanile750_denoised.png">
           <figcaption> The original Campanile image, the same image noised at timstep $t = 750$, and the result of 1 step denoising using stage_1.unet.</figcaption>
       </figure>
   
       <h2> 1.4 Iterative Denoising </h2>
       
       <p>
        In part 1.3 above, the 1-step denoising proves much more effective than using a Gaussian filter. However, the quality is notably worse for higher $t$.
        Here we implement iterative denoising using the same pretrained model and according to the followig formula:
       </p>
       
       <center>
        $x_{t'} = \frac{\sqrt{\bar{\alpha}_{t'}} \beta_t}{1 - \bar{\alpha}_t} x_0 
        \;+\; \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t'})}{1 - \bar{\alpha}_t} x_t 
            \;+\; v_\sigma$
       </center>
       
        <p>
        Where, $x_t$ is the image at timestep $t$, $x_{t'}$ is the the noisy image at the previous timestep $t' < t$, and $x_0$ is the current one-step estimate of the clean image.
        This is essentially a linear interpolation between the signal and the noise.
        Iteratively, we run this from $t=690$ to $t=0$ to obtain a denoised image. Below we see the process as $t$ decreeases from $t=690, t=540, t=390, t=90,$ and 
        finally $t=0$. Interestingly, the algorithm created a new top of the building but is otherwise convincing enough.
        </p>
   
        <figure class="line5">
            <img src="data/campanile_denoise690.png">
           <img src="data/campanile_denoise540.png">
           <img src="data/campanile_denoise390.png">
           <img src="data/campanile_denoise90.png">
            <img src="data/campanile_clean.png">
            <figcaption> The Campanile iteratively denoised starting from timestep $t=690$ down to $t=0$. </figcaption>
        </figure>
       
   
       <figure class="line3">
           <img src="data/campanile_clean.png">
           <img src="data/campanile_clean_one_step.png">
           <img src="data/campanile_clean_blur.png">
           <figcaption> On the left is the final iteratively-denoised image, compared to the single-step denoised image started from timestep $t=690$, and the Gaussian denoised image on the right.  </figcaption>
       </figure>
       
       <h2> 1.5 Diffusion Model Sampling </h2>
   
       <p>
       Here I generate images by starting with a fully randomly-noised image and setting i_start = 0 for the iterative denoising process with the prompt, "a high quality photo". The results
       of denoising 5 random noise samples are below.
        </p>


           <figure class="line5">
               <img src="data/generated_image_0.png">
               <img src="data/generated_image_1.png">
               <img src="data/generated_image_2.png">
               <img src="data/generated_image_3.png">
               <img src="data/generated_image_4.png">
                <figcaption> 5 images sampled from random noise. While some are nonsensical, two show distinct faces the model picked out from the noise. </figcaption>
           </figure>

   
       <h2> 1.6 Classifier-Free Guidance </h2>
        
       <p>
       In this section we use classifier-free guidance to improve the quality of the images generated by the diffusion model. In this approach,
       we compute a noise estimate conditioned on a text prompt as well as an unconditional noise estimate (generated by passing an empty prompt embedding to the model).
       Our new noise estimate is given by the equation below, 
        </p>

        <center>
            <p>
                \( \epsilon  = \epsilon_u + \gamma (\epsilon_c - \epsilon_u) \)
            </p>
        </center>

        <p>
            where \( \gamma \) controls the strength of the CFG. Here we set \( \gamma = 7 \) and generate five higher quality example photos below.
        </p>
   
           <figure class="line5">
               <img src="data/generated_image_cfg_0.png">
               <img src="data/generated_image_cfg_1.png">
               <img src="data/generated_image_cfg_2.png">
               <img src="data/generated_image_cfg_3.png">
               <img src="data/generated_image_cfg_4.png">
               <figcaption> 5 images sampled from the same model with CFG </figcaption>
           </figure>
   
    
    <h2> 1.7 Image-to-image translation</h2>
   
    <p>
       Now, to visualize the model’s creativity, I add noise to the Campanile image and iteratively denoise it using the conditional text prompt “a high-quality photo.” 
       This forces the model to hallucinate new details from an existing structure. As a result, we see (below) a sequence of images that increasingly resemble the original 
       Campanile. The image on the left is generated from the Campanile noised at timestep $t=960$ (nearly fully noised), and each subsequent image uses a slightly lower noise level. As the noise decreases, the denoised image gradually recovers the structure of the Campanile.
    </p>

       <figure class="line6">
           <img src="data/campanile_edited2_1.png">
           <img src="data/campanile_edited2_3.png">
           <img src="data/campanile_edited2_5.png">
           <img src="data/campanile_edited2_7.png">
           <img src="data/campanile_edited2_10.png">
           <img src="data/campanile_edited2_20.png">
           <figcaption> Campanile denoised at noise levels $t=960$, $t=900$, $t=840$, $t=780$, $t=690$, $t=960$, $t=390$ </figcaption>
       </figure>

       <p>
        <br>
       I repeat the exact same process with two of my sampled photos below.
        </p>
       <figure class="line6">
           <img src="data/testim_edited1_1.png">
           <img src="data/testim_edited1_3.png">
           <img src="data/testim_edited1_5.png">
           <img src="data/testim_edited1_7.png">
           <img src="data/testim_edited1_10.png">
           <img src="data/testim_edited1_20.png">
           <figcaption> "an Orca escaping SeaWorld" denoised at noise levels $t=960$, $t=900$, $t=840$, $t=780$, $t=690$, $t=960$, $t=390$  </figcaption>
       </figure>
   
       <figure class="line6">
           <img src="data/testim_edited2_1.png">
           <img src="data/testim_edited2_3.png">
           <img src="data/testim_edited2_5.png">
           <img src="data/testim_edited2_7.png">
           <img src="data/testim_edited2_10.png">
           <img src="data/testim_edited2_20.png">
           <figcaption> 'a Chinese city in ruins' denoised at noise levels $t=960$, $t=900$, $t=840$, $t=780$, $t=690$, $t=960$, $t=390$ </figcaption>
       </figure>
       
       <h3> 1.7.1 Editing Hand-Drawn and Web Images  </h3>
       
       <p> Here I repeat the process above with a web image of the Mona Lisa. This image is so circulated and reproduced that the model is able to predict it much
        earlier than previous examples.
       </p>
   
       <figure class="line6">
           <img src="data/web_edited_1.png">
           <img src="data/web_edited_3.png">
           <img src="data/web_edited_5.png">
           <img src="data/web_edited_7.png">
           <img src="data/web_edited_10.png">
           <img src="data/web_edited_20.png">
           <figcaption> The Mona Lisa denoised at noise levels $t=960$, $t=900$, $t=840$, $t=780$, $t=690$, $t=960$, $t=390$ </figcaption>
       </figure>
   
       <p>
        Below run it again with some hand drawn photos. The first photo I use is,
        </p>
   
        <center>
       <figure>
           <img src="data/drawnimage2.png" height="300" width="300">
           <figcaption style="color: #6b7280;"> The original image: a person standing in awe at the beauty of a rainbow.</figcaption>
       </figure>
        </center>

        <br>
        <p>
            In this first example, the model does a good job interpreting the photo, even adding tears to the
       subject along the way. However, it does not take it into the natural image manifold, perhaps because the rainbow is so rigidly defined.
        
        </p>
   
       <figure class="line6">
           <img src="data/drawnimage_edited_1.png">
           <img src="data/drawnimage_edited_3.png">
           <img src="data/drawnimage_edited_5.png">
           <img src="data/drawnimage_edited_7.png">
           <img src="data/drawnimage_edited_10.png">
           <img src="data/drawnimage_edited_20.png">
           <figcaption> Rainbow drawing denoised at noise levels $t=960$, $t=900$, $t=840$, $t=780$, $t=690$, $t=960$, $t=390$ </figcaption>
       </figure>
   
       <br>
       <p> 
            For the second example, I tried to keep more vague figures and lines in order to 
            allow the model to interpret more freely. However, I think the cartoonish colors (and debatably poorly drawn photos) make it difficult to direct the model.
        </p>
        
        <center>
            <figure>
                <img src="data/drawnimage.png" height="300" width="300">
                <figcaption style="color: #6b7280;"> The original image: (roughly) three figures playing basketball in the sun </figcaption>
            </figure>
        </center>

   
       <figure class="line6">
           <img src="data/drawnimage0_edited_1.png">
           <img src="data/drawnimage0_edited_3.png">
           <img src="data/drawnimage0_edited_5.png">
           <img src="data/drawnimage0_edited_7.png">
           <img src="data/drawnimage0_edited_10.png">
           <img src="data/drawnimage0_edited_20.png">
           <figcaption> Vague drawing denoised at noise levels $t=960$, $t=900$, $t=840$, $t=780$, $t=690$, $t=960$, $t=390$ </figcaption>
       </figure>
   
       <h3> 1.7.2 Inpainting </h3>
       
       <p>
        Using the RePaint-style inpainting procedure, I edited the top of the Campanile by running the diffusion denoising loop while constraining all unmasked pixels
         to remain consistent with the original image. At each timestep, after predicting the denoised image, I overwrote the unedited regions with the corresponding 
         pixels from the original image, properly noised for that timestep, while allowing the masked region to be freely generated by the model.
          This iterative process produced a final image where only the selected area was newly synthesized, and the rest of the photograph remained unchanged.
          The procedure is defined by the equation below,
        </p>
       <br>

       <center>
       <p>
           \( x_t = mx_t + (1-m)*forward(x_{orig}, t) \)
       </p>
        </center>

        And the results,
       
       <br>
       <center>
       <figure>
            <img src="data/inpainted_image.png" height ="300" width="300">
       </figure>
     </center>
       
        <p>
            Also create custom masks for two test images below. What might the model make sense of the space where the rats should be in the 'owl teaching rats photo'?
        </p>
   
       <figure class="line2">
           <img src="data/generated_image_stage3_2.png">
           <img src="data/inpainted_image1.png">
           <figcaption> The RePaint method applied to the owl's photo. </figcaption>
       </figure>
       <br>
       <p>
       Or, instead of creating two whales, what would it do with the extra space given an opportunity to re-render?
        </p>
       <figure class="line2">
           <img src="data/generated_image_stage3_10.png">
           <img src="data/inpainted_image2.png">
           <figcaption> The RePaint method applied to the Orca photo. </figcaption>
       </figure>
   
        <h3>1.7.3 Text-Conditional Image-to-image Translation </h3> 
        
        <p>
            Now, instead of using a null prompt embedding, I guide the projection with a text prompt to see two images converge over iterations.
        </p>
        
   
       <figure class="line7">
           <img src="data/milk_camp1.png"> 
           <img src="data/milk_camp3.png"> 
           <img src="data/milk_camp5.png"> 
           <img src="data/milk_camp7.png"> 
           <img src="data/milk_camp10.png"> 
           <img src="data/milk_camp20.png"> 
           <img src="data/campanile.jpg">
           
           <figcaption> 
                Here "a jug of milk" is tranformed into the Campanile by denoising the Campanile at noise levels $t=960$, $t=900$, $t=840$, $t=780$, $t=690$, $t=960$, $t=390$, 
                compared to the original image.
           </figcaption>
       </figure>
   
       <figure class="line7">
           <img src="data/build_whale1.png"> 
           <img src="data/build_whale3.png"> 
           <img src="data/build_whale5.png"> 
           <img src="data/build_whale7.png"> 
           <img src="data/build_whale10.png"> 
           <img src="data/build_whale20.png"> 
           <img src="data/generated_image_stage3_10.png">
           
           <figcaption> Here "two identical black and white skyscrapers" are tranformed into the Campanile by denoising the Campanile at noise levels $t=960$, $t=900$, $t=840$, $t=780$, $t=690$, $t=960$, $t=390$,
            compared to the original image.
           </figcaption>
       </figure>
   
       <figure class="line7">
           <img src="data/china_china1.png"> 
           <img src="data/china_china3.png"> 
           <img src="data/china_china5.png"> 
           <img src="data/china_china7.png"> 
           <img src="data/china_china10.png"> 
           <img src="data/china_china20.png"> 
           <img src="data/generated_image_stage3_2.png">
           
           <figcaption> Here "a teacher teaching a class full of students" is tranformed into the Campanile by denoising the Campanile at noise levels $t=960$, $t=900$, $t=840$, $t=780$, $t=690$, $t=960$, $t=390$,
            compared to the original image.
         </figcaption>
       </figure>

       <h2>1.8 Visual anagrams</h2>
       
       <p>
       Here I utilized the diffusion to create optical illusions. 
       At each denoising step, I ran the UNet twice: once on the current noisy image with the first prompt, and once on the vertically flipped image with the 
       second prompt. I then flipped the second output back, averaged the two noise estimates, and used this averaged noise for the reverse diffusion 
       update. Iterating this process produced a single image that encodes both prompts, depending on its orientation. Mathematically, we replace our noise
       estimate with the one below,
        </p>
   
        <center> \( \epsilon_1 = CFG of UNet(x_t, t, p_1) \) <br>
       
       \( \epsilon_2 = Flip(CFG of UNet(flip(x_t), t, p_2) \) <br>
   
       \( \epsilon = (\epsilon_1 + \epsilon_2)/2 \)
     </center>
   
       <figure class="line2">
           <img src="data/illusion.png">
           <img src="data/illusion.png" style="transform: rotate(180deg);" />
           <figcaption> Right side up: an oil painting of two faces touching, and upside down: an oil painting of two blooming flowers </figcaption>
       </figure>

       <br>
   
       <figure class="line2">
           <img src="data/illusion1.png">
           <img src="data/illusion1.png" style="transform: rotate(180deg);" />
           <figcaption> Right side up: an oil painting of two whales kissing underwater, and upside down: an oil painting of butterfly wings </figcaption>
       </figure>
   
       <h2> 1.9 Hybrid images </h2>
       
       <p>
       Using Factorized Diffusion, I created hybrid images by combining low- and high-frequency information from two different text prompts. 
       At each denoising step, I ran the UNet twice with two prompts to obtain two noise estimates, then applied a Gaussian blur (kernel size 33, sigma 2)
        to extract low frequencies from one estimate and high frequencies from the other. I added these together to form a composite noise estimate, which I 
        used in the reverse diffusion update. Repeating this process produced a single hybrid image that simultaneously reflects both prompts at different spatial
         frequency scales. Mathematically, the new noise estimate is given by,
        </p>
       
       <br>

       <center>
   
       \( \epsilon_1 = CFG of UNet(x_t, t, p_1) \) <br>
       \( \epsilon_2 = CFG of UNet(x_t, t, p_2) \) <br>
   
       \( \epsilon = highpass( \epsilon_1 ) + lowpass( \epsilon_2 ) \) <br>
        </center>
        <br>
   
        To encourage better results (especially at 64x64 images),
       I made prompts that emphasized low and high features respecitvely. For example, a high frequency prompt might emphasize wrinkles, sharp lines, etc. while a low frequency
       prompt should emphasize smoothness, low contract elements. I made prompts that would intentially create overlapping shapes for a convincing result. There are two
       examples below.
        
       <center>
       <figure>
           <img src="data/hybrid_image.png" height="300" width="300">
           <figcaption style="color: #6b7280;"> 
               High frequency prompt: "a frowning old man's wrinkly face, deep wrinkles, very sharp details, high contrast, gritty texture"
               Low frequency prompt: "a smiling young woman's face, soft lighting, smooth skin, low contrast, slightly blurred portrait"
           </figcaption>
       </figure>
      </center>
   
       <figure class="line3">
           <img src="data/hybrid_image2.png">
           <img src="data/hybrid_image3.png">
           <img src="data/hybrid_image4.png">
           <figcaption> 
                Here are three generated examples from different random noise samples using the same prompts.
               High frequency prompt: "close-up of dried rose petals with sharp micro-details, deep wrinkles"
               Low frequency prompt: "a canyon scene with large red and orange cliffs, deep shadows, smooth rocks, and broad shapes"
           </figcaption>
       </figure>
       
       <br>

       <h2> Bells and Whistles: More Visual Anagrams </h2>

       <p> 
          Here I implement two additional illusions using methods described in <a href = "https://arxiv.org/pdf/2311.17919"> this paper.</a>
       </p>

       <h3> Four-view visual anagrams </h3>
       <p>
        First, similar to the flipped image above, I used the model to create image with using four different text emeddings that can be viewed differently from four 
        diffrent angles. At each denoising step, the UNet ran 4 times, once per each of the 4 rotations (0deg, 90deg, 180deg, and 270deg) of the noisy image. Each 
        rotated image was paired with it's prompt when passed the model for a noise estimate. I then average all four noise estimates for the diffusion update.
        Doing so encoded four different prompts into the same image, each hopefully becoming dominate when viewed from its cooresponding rotation angle. Mathematically,
        the noise estimate calculated by these equations,
       </p>

       <center>
        \( \epsilon_1 = CFG\big(\mathrm{UNet}(x_t,\; t,\; p_1)\big) \) <br><br>
      
        \( \epsilon_2 = \mathrm{RotateBack}_{90^\circ}\!\Big(CFG\big(\mathrm{UNet}(\mathrm{Rotate}_{90^\circ}(x_t),\; t,\; p_2)\big)\Big) \) <br><br>
      
        \( \epsilon_3 = \mathrm{RotateBack}_{180^\circ}\!\Big(CFG\big(\mathrm{UNet}(\mathrm{Rotate}_{180^\circ}(x_t),\; t,\; p_3)\big)\Big) \) <br><br>
      
        \( \epsilon_4 = \mathrm{RotateBack}_{270^\circ}\!\Big(CFG\big(\mathrm{UNet}(\mathrm{Rotate}_{270^\circ}(x_t),\; t,\; p_4)\big)\Big) \) <br><br>
      
        \( \epsilon = \tfrac{1}{4}(\epsilon_1 + \epsilon_2 + \epsilon_3 + \epsilon_4) \) <br> <br>
      </center>

      <p>
        Using the prompt embeddings for "an oil painting of beautiful flowers", "an oil painting of a giraffe", "an oil painting of woman's face", and "an oil painting of a dog",
        here are three example outputs from the model. Interestingly, some prompts sometimes dominate the image while some are sometimes harder to notice. 
      </p>
      
       <div class="line4">
          <figure>
            <img src="data/illusion_4flips.png">
            <figcaption>an oil painting of beautiful flowers</figcaption>
          </figure>

          <figure>
            <img src="data/illusion_4flips.png" style="transform: rotate(90deg);">
            <figcaption>an oil painting of a giraffe</figcaption>
          </figure>

          <figure>
            <img src="data/illusion_4flips.png" style="transform: rotate(180deg);">
            <figcaption>an oil painting of woman's face</figcaption>
          </figure>

          <figure>
            <img src="data/illusion_4flips.png" style="transform: rotate(270deg);">
            <figcaption>an oil painting of a dog</figcaption>
          </figure>
       </div>

       <div class="line4">
        <figure>
          <img src="data/illusion_4flips2.png">
          <figcaption>an oil painting of beautiful flowers</figcaption>
        </figure>

        <figure>
          <img src="data/illusion_4flips2.png" style="transform: rotate(90deg);">
          <figcaption>an oil painting of a giraffe</figcaption>
        </figure>

        <figure>
          <img src="data/illusion_4flips2.png" style="transform: rotate(180deg);">
          <figcaption>an oil painting of woman's face</figcaption>
        </figure>

        <figure>
          <img src="data/illusion_4flips2.png" style="transform: rotate(270deg);">
          <figcaption>an oil painting of a dog</figcaption>
        </figure>
     </div>

     <div class="line4">
      <figure>
        <img src="data/illusion_4flips3.png">
        <figcaption>an oil painting of beautiful flowers</figcaption>
      </figure>

      <figure>
        <img src="data/illusion_4flips3.png" style="transform: rotate(90deg);">
        <figcaption>an oil painting of a giraffe</figcaption>
      </figure>

      <figure>
        <img src="data/illusion_4flips3.png" style="transform: rotate(180deg);">
        <figcaption>an oil painting of woman's face</figcaption>
      </figure>

      <figure>
        <img src="data/illusion_4flips3.png" style="transform: rotate(270deg);">
        <figcaption>an oil painting of a dog</figcaption>
      </figure>
   </div>


   <h3> Two-view negatives </h3>
   
   <p>
      To create the negative optical illusion, I modify the denoising process similar to the above example. Each timestep uses both the noisy image
      and its negative. Using the first prompt, the UNet predicts noise under the first promp and likewise for the second prompt. I negate 
      the second noise estimate again, and use their average as my noise estimte, according to the equations below,
   </p>
    <center>
      \( \epsilon_1 = CFG(\mathrm{UNet}(x_t,\; t,\; p_1)) \) <br><br>
      \( \epsilon_2 = -CFG(\mathrm{UNet}(-x_t,\; t,\; p_2)) \) <br><br>
      \( \epsilon = \tfrac{1}{2}(\epsilon_1 + \epsilon_2) \)
    </center>

    <p>
      Below we see 3 examples, where the negative of one image shows a completely different prompt.
    </p>

   <div class="line2">
    <figure>
      <img src="data/illusion_neg.png">
      <figcaption>an oil painting of beautiful flowers</figcaption>
    </figure>

    <figure>
      <img src="data/illusion_neg2.png">
      <figcaption>an oil painting of a dog</figcaption>
    </figure>
   </div>

   <div class="line2">
    <figure>
      <img src="data/illusion_neg3.png">
      <figcaption>an oil painting of woman's face</figcaption>
    </figure>

    <figure>
      <img src="data/illusion_neg4.png">
      <figcaption>an oil painting of a giraffe</figcaption>
    </figure>
   </div>

   <div class="line2">
    <figure>
      <img src="data/illusion_neg5.png">
      <figcaption>an oil painting of a dog</figcaption>
    </figure>

    <figure>
      <img src="data/illusion_neg6.png">
      <figcaption>an oil painting of a giraffe</figcaption>
    </figure>
   </div>

   <h2> Class logo </h2>

   <p>
      Here I implement text-conditioned image-to-image translation on the image of the Campanile with the prompt "the Berkeley Campanile drawn as a neural network graph, glowing connections, blue and gold nodes, futuristic look". 
      The best results appeared at high timesteps (800-900), and show a nice animated yet recognizable Campanile with a "Neural Network" theme that
      represents the theme of this class well. Five example outputs are below.
   </p>

   <figure class="line5">
      <img src="data/class_logo3.png">
      <img src="data/class_logo5.png">
      <img src="data/class_logo3_.png">
      <img src="data/class_logo5_.png">
      <img src="data/class_logo10_.png">
      <figcaption> Class Logo outputs </figcaption>
   </figure>
      
       <h1> Part 2: Flow Matching From Scratch </h1>

       <p>
        Here I implemented a UNet defined by the following architecture,
        </p>

       <figure class="figure">
            <img src="data/UNet_architecture.png">
       </figure>

       Where,

       <figure class="figure">
            <img src="data/ops.png">
        </figure>

        <p>
            At a high level, the blocks do the following,
        </p>

        <ul>
            <li>Conv is a convolutional layer that doesn't change the image resolution, only the channel dimension.</li>
            <li>DownConv is a convolutional layer that downsamples the tensor by 2.</li>
            <li>UpConv is a convolutional layer that upsamples the tensor by 2.</li>
            <li>Flatten is an average pooling layer that flattens a 7x7 tensor into a 1x1 tensor. 7 is the resulting height and width after the downsampling operations.</li>
            <li>Unflatten is a convolutional layer that unflattens/upsamples a 1x1 tensor into a 7x7 tensor.</li>
            <li>Concat is a channel-wise concatenation between tensors with the same 2D shape.</li>
        </ul>
   
       <h2> 1.2 Using the UNet to Train a Denoiser </h2>
       
       <p>
        To train my denoiser, I generate training pairs (x, z) where x is a clean image from the MNIST dataset and z is x with sampled noise added, via this process:
       </p>
       
       <center>
       \( z = x + \sigma \epsilon \), where \( \epsilon \sim N(0, I) \)
       </center>
   
       <p>
       I will define loss as the MSE between x and z,
        </p>
        
        <center>
       \( L = E_{z,x}||D_{\theta}(z) - x||^2 \)
        </center> <br>
       
        <p>
        Here we visualize the noising process over different \( \sigma \in [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0] \)
        </p>
   
       <figure class = "line7">
           <img src="data/noisy_image_0.png">
           <img src="data/noisy_image_1.png">
           <img src="data/noisy_image_2.png">
           <img src="data/noisy_image_3.png">
           <img src="data/noisy_image_4.png">
           <img src="data/noisy_image_5.png">
           <img src="data/noisy_image_6.png">
       </figure>
   
       <h2>1.2.1  Training </h2>
       
       <p>
        Here I train the model with a noise level of 0.5 for 5 epochs with a batch size of 256. The MSE training loss curve is below, along with three 
       example inputs and outputs.
       </p>
   
       <figure class="figure">
            <img src="data/training_curve.png">
            <figcaption> Training curve </figcaption>
        </figure>

       <figure class="line2">
           <img src="data/test_ex1_noisy.png">
           <img src= "data/test_nonpure1_epoch1.png">
           <figcaption> A noisy test image (7), denoising results after 5 epochs. </figcaption>
       </figure>
   
       <figure class="line2">
           <img src="data/test_ex2_noisy.png">
           <img src= "data/test_nonpure2_epoch1.png">
           <figcaption> A noisy test image (2), denoising results after 5 epochs. </figcaption>
       </figure>
   
       <figure class="line2">   
           <img src="data/test_ex3_noisy.png">
           <img src= "data/test_nonpure3_epoch1.png">
           <figcaption> A noisy test image (1), denoising results after 5 epochs. </figcaption>
       </figure>
   
       
   
       <h2>1.2.2 Out-of-Distribution Testing</h2>
   
       <p>
       Since the UNet was trained with a noise level of 0.5, we can't expect it to generalize to all noise levels. Here we see three test images noised 
       at each value \( \sigma \in \) [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]. Each row below cooresponds to each increasing noise level, and within in row I show
       how the model performs across the three test images. As you can see, the model has reasonable performance even at a high noise level.
        </p>
       <figure class = "line3">
           <img src="data/test_oodt1_0.png">
           <img src="data/test_oodt2_0.png">
           <img src="data/test_oodt3_0.png">
           <figcaption> \( \sigma = 0.0 \)</figcaption>
       </figure>
   
       <figure class = "line3">
           <img src="data/test_oodt1_1.png">
           <img src="data/test_oodt2_1.png">
           <img src="data/test_oodt3_1.png">
           <figcaption> \( \sigma = 0.2 \)</figcaption>
       </figure>
   
       <figure class = "line3">
           <img src="data/test_oodt1_2.png">
           <img src="data/test_oodt2_2.png">
           <img src="data/test_oodt3_2.png">
           <figcaption> \( \sigma = 0.4 \)</figcaption>
       </figure>
   
       <figure class = "line3">
           <img src="data/test_oodt1_3.png">
           <img src="data/test_oodt2_3.png">
           <img src="data/test_oodt3_3.png">
           <figcaption> \( \sigma = 0.5 \)</figcaption>
       </figure>
   
       <figure class = "line3">
           <img src="data/test_oodt1_4.png">
           <img src="data/test_oodt2_4.png">
           <img src="data/test_oodt3_4.png">
           <figcaption> \( \sigma = 0.6 \)</figcaption>
       </figure>
       
       <figure class = "line3">
           <img src="data/test_oodt1_5.png">
           <img src="data/test_oodt2_5.png">
           <img src="data/test_oodt3_5.png">
           <figcaption> \( \sigma = 0.8 \)</figcaption>
       </figure>
   
       <figure class = "line3">
           <img src="data/test_oodt1_6.png">
           <img src="data/test_oodt2_6.png">
           <img src="data/test_oodt3_6.png">
           <figcaption> \( \sigma = 1.0 \)</figcaption>
       </figure>
   
       <h2>1.2.3 Denoising Pure Noise</h2>
       
       <p>
       To make denoising a general task, I will retrain the model where \( z = \epsilon \) and \( \epsilon \sim N(0, I) \), and denoising to get a clean \( x \). Below,
       we see the denoising results on 3 pure noise after the first epoch, the fifth epoch, and the training curve. The results for each image are 
       nearly identical. This task is distinct from the first since the inputs now carry zero information about the targets, whereas before the target number in the input was visible
       to the human eye. The result is that the outputs appear to be a rough average of all digits. This makes sense, since random noise is trained to map 
       to all digits. With MSE loss, the model learns to predict the point that minimizes the sum of squared distances to all training examples.
       </p>
       <figure class="line3">
           <img src = "data/test_pure1_epoch1.png">
           <img src = "data/test_pure2_epoch1.png">
           <img src = "data/test_pure3_epoch1.png">
           
           <figcaption> The result of denoising 3 pure noise samples after 1 epoch </figcaption>
       </figure>
   
       <figure class="line3">
            <img src = "data/test_pure1_epoch5.png">
           <img src = "data/test_pure2_epoch5.png">
           <img src = "data/test_pure3_epoch5.png">
           <figcaption> The result of denoising 3 pure noise samples (same as above) after 5 epochs </figcaption>
       </figure>

       <figure class="figure">
           <img src="data/training_curve2.png">
           <figcaption> Training curve on pure noise </figcaption>
       </figure>
   
       <h2> Part 2: Training a Flow Matching Model </h2>
   
       <p>
           As one step denoising proved inaccurate, here I implement iterative denoising with flow matching. Here, I sample a pure noise image \( x_0 \sim N(0,I) \) to
           generate a realistic image \( x_1 \). I do so by a linear interpolation between \( x_0 \) and \( x_1 \): <br>
        </p>
        
        <center>
           \( x_t = (1-t)x_0 + tx_1 \), where \( t \in [0,1] \) <br><br>
        </center>
        <p>
           The result is a vector field describing a point \( x_t \) at time \( t \) in flow and relative to both the clean and noisy data distributions. We can think of
           this flow as a the velocity of the vector field descrbing how to move from \( x_0 \) to \( x_1 \). <br>
        </p>
        
        <center>
           \( u(x_t, t) = d/dt x_t = x_1 - x_0 \) <br> <br>
        </center>
   
        <p>
           Here I train a UNet \( u_{\theta}(x_t, t) \) that approximates this flow/velocity, thus allowing predictive denoising.
        </p>
        <center>
           \( L = E_{\theta} ||(x_1 - x_0) - u_{\theta}(x_t, t)||^2 \) <br><br>
        </center>
   
       <h3> 2.1 Adding Time Conditioning to UNet </h3>
   
           The structure of the time conditioned UNet is similar to the unconditional UNet, with a scalar \( t \) injected as shown in the diagram below.
   
           <img src="data/t_condtioning.png">
   
       <h2> 2.2 Below is the training curve for the time conditioned UNet over 10 epochs. </h2>
        
       <center>
        <img src="data/training_curve2.png">
       </center>
       
   
       <h2> 2.3 Sampling from the UNet </h2>
   
       Now, to sample from the UNet, we take a pure noise image \( x_0 = x_t\) and run it through 100 timesteps. In each step, we push \( x_t \) in the direction (given by the UNet)
       of a denoised image. Below are 5 samples from the UNet trained for 1 epoch, 5 epochs, and 10 epochs. At the first epoch, the images do not appear noisy, but 
       neither do they resemble digits. After the 5th, and even better after the 10th epoch, we start to see recognizable digits emerge.
   
       <figure class="line5">
           <img src="data/test_pure0_epoch1.png">
           <img src="data/test_pure1_epoch1.png">
           <img src="data/test_pure2_epoch1.png">
           <img src="data/test_pure3_epoch1.png">
           <img src="data/test_pure4_epoch1.png">
           <figcaption> 5 sample images from the UNet trained for 1 epoch </figcaption>
       </figure>
   
       <figure class="line5">
           <img src="data/test_pure0_epoch5.png">
           <img src="data/test_pure1_epoch5.png">
           <img src="data/test_pure2_epoch5.png">
           <img src="data/test_pure3_epoch5.png">
           <img src="data/test_pure4_epoch5.png">
           <figcaption> 5 sample images from the UNet trained for 5 epochs </figcaption>
       </figure>
   
       <figure class="line5">
           <img src="data/test_pure0_epoch10.png">
           <img src="data/test_pure1_epoch10.png">
           <img src="data/test_pure2_epoch10.png">
           <img src="data/test_pure3_epoch10.png">
           <img src="data/test_pure4_epoch10.png">
           <figcaption> 5 sample images from the UNet trained for 10 epochs </figcaption>
       </figure>
   
       <h2> Bells and whistles: Improving the time conditioned UNet </h2>
   
       <p> 
           While these results are denoised, they hardly represent real numbers. In fact, only a few samples above may be classified as a handwritten digit. 
           It makes sense that without class labels the model learns a single velocity field that leads random noise to blurred or averaged digit shapes. <br>
           
           To improve the results, I first increased the complexity of the model by doubling the hidden dimensions from 64 to 128, thus giving the UNet more capacity 
           to model the MNIST distribution. I thought that this may decrease the averaging of digits in the result by allowing more differentiation. <br>
           
           I also increased the number of epochs from 10 to 25, allowing the model more time to converge. I increased the number of timesteps to 150, allowing
           the model to spend more time breaking apart the noise.
   
           Together, these changes produces signifcantly more recognizable digits, as shown below with 10 additional samples.
       </p>
   
       <figure class="line5">
           <img src ="data/tc_unet_25epochs_sample0.png">
           <img src ="data/tc_unet_25epochs_sample1.png">
           <img src ="data/tc_unet_25epochs_sample2.png">
           <img src ="data/tc_unet_25epochs_sample3.png">
           <img src = "data/tc_unet_25epochs_sample4.png">
       </figure>
   
       <figure class="line5">
           <img src ="data/tc_unet_25epochs_sample5.png">
           <img src ="data/tc_unet_25epochs_sample6.png">
           <img src ="data/tc_unet_25epochs_sample7.png">
           <img src ="data/tc_unet_25epochs_sample8.png">
           <img src ="data/tc_unet_25epochs_sample9.png">
       </figure>
   
       <h2> 2.4 Class Conditioning </h2>
   
       <p>
       To improve the results, I add class conditioning to the model, where each class cooresponds to a class of digit 0-9. Similarly to how I added the time conditioning,
       I add two more FCBlocks to the UNet that incorporate the one-hot class-conditioning vector, \( c \). Because I still want this UNet to work without class conditioning,
       I add a dropout rate of 10% where the class conditioning vector is set to 0.
        </p>
   
       <h2> 2.5 Training the UNet </h2>

       <p>
       Below is a graph of the training curve from the class-conditioned UNet over 10 epochs. <br>
        </p>
        
        <center>
          <img src="data/training_curve4.png">
        </center>
       
   
       <h2> 2.6 Sampling from the UNet </h2>
        
       <p>
       Below, I show sample results from the class-encoding and classifier-free guidance with a guidance scale of 5. Results are shown after 1, 5, and 10 epochs. The first
       result is still very grainy with very deformed digits. However, by the fifth epoch, the results are entirely convincing with only minor improvements by the 10th epoch.
        </p>

       <figure class="figure">
           <img src="data/5x10_samples1.png" width="1000" height="600">
           <figcaption> Results 5 of each digit class from class encoded, cfg UNet after 1 epoch </figcaption>
       </figure>
   
       <figure class="figure">
           <img src="data/5x10_samples2.png" width="1000" height="600">
           <figcaption> Results 5 of each digit class from class encoded, cfg UNet after 5 epochs </figcaption>
       </figure>
   
       <figure class="figure">
           <img src="data/5x10_samples3.png" width="1000" height="600">
           <figcaption> Results 5 of each digit class from class encoded, cfg UNet after 10 epochs </figcaption>
       </figure>
       
       <p>
           Finally, to make the model simpler, we experiment with removing the learning rate scheduler. Right now the learning rate starts at 1e-2. The scheduler
           is initialized with \( \gamma = 0.1^{1/10} = 0.79 \). By the tenth epoch, \( 0.79^{10} = 0.1 \), and thus the learning rate decays to roughly 1e-3 by the tenth
           epoch. 
       </p>
       <p>
           As you can see in the first figure below, after removing the scheduler, the results remain very strong. However, the 4 in row 0, the 2 in row 3, and the 7 and 8 in row 1 (0 based rows)
           are examples where the model performance declines. We also see some random splotches of white by several images. This suggests that the algorithm is still
           reaching a reasonable local minimum region for the loss, but due to the high learning rate, does not refine into the actual local minimum. 
       </p>
       <p>
           To account for removing the scheduler, I trained the model with a lower learning rate of 1e-3 and increased the epochs from 10 to 15 in order to allow more
           time to explore and ultimately acheive a local minimum. The results, shown in the final figure below, are then comparable to the model trained with the scheduler, with only minor
           issues such as seveal 6's missing holes and some auxilary faint dots. We no longer see deformed numbers as in the first result.
       </p>
   
       <figure class="figure">
           <img src ="data/5x10_samples_test.png" width="1000" height="600">
           <figcaption> Results after removing the lr scheduler </figcaption>
       </figure>
   
       <figure class="figure">
           <img src ="data/5x10_samples_test2.png" width="1000" height="600">
           <figcaption> Results after removing the lr scheduler, lowering the learning rate to 1e-3, and increasing the epochs from 10 to 15 </figcaption>
       </figure>

       </article>
    </div>

      <!-- Sidebar 
      <aside class="sidebar">
        <p class="sidebar-title"> Content </p>
        <ul class="toc">
          <li>
            <a href="#start">Part A: The Power of Diffusion Models</a>
            <ul class="toc-sub">
              <li><a href="#part-0">Part 0: Setup</a></li>
              <li><a href="#part-1">Part 1: Sampling Loop</a></li>
              <li><a href="#part-b">Part B: Flow Matching</a></li>
            </ul>
          </li>
        </ul>
      </aside>
    </main> -->

   

</body>
</html>